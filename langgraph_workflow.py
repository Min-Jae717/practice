from typing import TypedDict, Annotated, List, Dict, Optional, Union
import operator
import streamlit as st
import numpy as np
from datetime import datetime, timedelta
import requests
from urllib.parse import urlencode, quote_plus

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from database import BidDataManager, VectorSearchManager, convert_to_won_format
from ai_search import SemanticSearchEngine

# OpenAI API í‚¤
OPENAI_API_KEY = "your-openai-api-key"  # ì‹¤ì œ API í‚¤ë¡œ ë³€ê²½

# ë‚˜ë¼ì¥í„° API ì„¤ì •
API_KEY = "your-narajangter-api-key"  # ì‹¤ì œ API í‚¤ë¡œ ë³€ê²½
BASE_URL_COMMON = "http://apis.data.go.kr/1230000/ad/BidPublicInfoService"
API_ENDPOINTS = {
    "ê³µì‚¬": f"{BASE_URL_COMMON}/getBidPblancListInfoCnstwk",
    "ìš©ì—­": f"{BASE_URL_COMMON}/getBidPblancListInfoServc", 
    "ë¬¼í’ˆ": f"{BASE_URL_COMMON}/getBidPblancListInfoThng",
    "ì™¸ì": f"{BASE_URL_COMMON}/getBidPblancListInfoFrgcpt",
}

# ========== LangGraph ìƒíƒœ ì •ì˜ ==========
class BidSearchState(TypedDict):
    """ì…ì°° ê³µê³  ê²€ìƒ‰ ìƒíƒœ"""
    query: str
    search_type: str
    category_filter: Optional[List[str]]
    date_range: Optional[tuple]
    supabase_results: List[Dict]
    vector_results: List[Dict]
    api_results: Dict[str, List[Dict]]
    combined_results: List[Dict]
    final_answer: str
    error: Optional[str]
    status_messages: Annotated[List[str], operator.add]
    quality_score: float
    expanded_query: str
    need_api_search: bool
    total_count: int

class HybridSearchState(TypedDict):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ì˜ ìƒíƒœ"""
    query: str
    search_method: List[str]
    supabase_results: List[Dict]
    vector_results: List[Dict]
    api_results: Dict[str, List[Dict]]
    combined_results: List[Dict]
    final_results: List[Dict]
    summary: str
    messages: Annotated[List[Union[HumanMessage, AIMessage]], operator.add]
    error: Optional[str]
    total_count: int
    need_api_search: bool

# ========== LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ ==========
def preprocess_query_node(state: BidSearchState) -> BidSearchState:
    """ì¿¼ë¦¬ ì „ì²˜ë¦¬ ë…¸ë“œ"""
    try:
        query = state["query"].lower()
        
        # ì…ì°° ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥
        keyword_expansions = {
            "ai": ["ì¸ê³µì§€ëŠ¥", "AI", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹"],
            "ì„œë²„": ["ì„œë²„", "ì„œë²„êµ¬ì¶•", "ì„œë²„ì‹œìŠ¤í…œ", "ì¸í”„ë¼"],
            "sw": ["ì†Œí”„íŠ¸ì›¨ì–´", "SW", "S/W", "í”„ë¡œê·¸ë¨"],
            "hw": ["í•˜ë“œì›¨ì–´", "HW", "H/W", "ì¥ë¹„"],
            "ì‹œìŠ¤í…œ": ["ì‹œìŠ¤í…œ", "ì‹œìŠ¤í…œêµ¬ì¶•", "ì •ë³´ì‹œìŠ¤í…œ"],
            "ê°œë°œ": ["ê°œë°œ", "êµ¬ì¶•", "ì œì‘", "ê°œë°œì‚¬ì—…"]
        }
        
        expanded_terms = [query]
        for key, synonyms in keyword_expansions.items():
            if key in query:
                expanded_terms.extend(synonyms)
        
        state["expanded_query"] = " ".join(set(expanded_terms))
        state["status_messages"] = [f"âœ… ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(expanded_terms)}ê°œ ê²€ìƒ‰ì–´"]
        
    except Exception as e:
        state["error"] = f"ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"
        
    return state

def search_supabase_node(state: BidSearchState) -> BidSearchState:
    """Supabase ê²€ìƒ‰ ë…¸ë“œ"""
    try:
        bid_manager = BidDataManager()
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰
        categories = state.get("category_filter")
        start_date = None
        end_date = None
        
        if state.get("date_range"):
            start_date, end_date = state["date_range"]
            start_date = start_date.strftime("%Y%m%d") if start_date else None
            end_date = end_date.strftime("%Y%m%d") if end_date else None
        
        results = bid_manager.search_bids_by_keyword(
            state.get("expanded_query", state["query"]),
            categories,
            start_date,
            end_date
        )
        
        state["supabase_results"] = results
        state["status_messages"] = [f"âœ… Supabaseì—ì„œ {len(results)}ê°œ ê³µê³  ê²€ìƒ‰ ì™„ë£Œ"]
        
    except Exception as e:
        state["error"] = f"Supabase ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
        state["supabase_results"] = []
        
    return state

def search_vector_db_node(state: BidSearchState) -> BidSearchState:
    """ë²¡í„° DB ì‹œë§¨í‹± ê²€ìƒ‰ ë…¸ë“œ"""
    try:
        semantic_engine = SemanticSearchEngine(OPENAI_API_KEY)
        
        # ì‹œë§¨í‹± ê²€ìƒ‰ ìˆ˜í–‰
        results = semantic_engine.search(
            state["query"], 
            num_results=30,
            similarity_threshold=0.3
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        vector_results = []
        for metadata, similarity in results:
            vector_results.append({
                "metadata": metadata,
                "similarity": similarity,
                "bidNtceNo": metadata.get('ê³µê³ ë²ˆí˜¸', 'N/A'),
                "bidNtceNm": metadata.get('ê³µê³ ëª…', 'N/A'),
                "ntceInsttNm": metadata.get('ê¸°ê´€ëª…', 'N/A')
            })
        
        state["vector_results"] = vector_results
        state["status_messages"] = [f"âœ… ë²¡í„° DBì—ì„œ {len(vector_results)}ê°œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ"]
        
    except Exception as e:
        state["error"] = f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
        state["vector_results"] = []
        
    return state

def combine_results_node(state: BidSearchState) -> BidSearchState:
    """ê²°ê³¼ í†µí•© ë…¸ë“œ"""
    try:
        supabase_results = state.get("supabase_results", [])
        vector_results = state.get("vector_results", [])
        combined_dict = {}

        # Supabase ê²°ê³¼ ì¶”ê°€
        for item in supabase_results:
            bid_no = item.get("bidNtceNo")
            if bid_no:
                combined_dict[bid_no] = {
                    **item,
                    "source": "Supabase",
                    "relevance_score": 5.0
                }

        # ë²¡í„° ê²°ê³¼ ì¶”ê°€/ë³‘í•©
        for item in vector_results:
            bid_no = item.get("bidNtceNo")
            if bid_no:
                if bid_no in combined_dict:
                    combined_dict[bid_no]["relevance_score"] += item["similarity"] * 10
                    combined_dict[bid_no]["similarity"] = item["similarity"]
                    combined_dict[bid_no]["source"] += ", Vector"
                else:
                    # Supabaseì—ì„œ ì¶”ê°€ ì •ë³´ ì¡°íšŒ
                    bid_manager = BidDataManager()
                    bid_detail = bid_manager.get_bid_by_number(bid_no)
                    
                    if bid_detail:
                        raw_data = bid_detail.get('raw', {})
                        combined_dict[bid_no] = {
                            "bidNtceNo": bid_no,
                            "bidNtceNm": raw_data.get('bidNtceNm', 'N/A'),
                            "ntceInsttNm": raw_data.get('ntceInsttNm', 'N/A'),
                            "bsnsDivNm": raw_data.get('bsnsDivNm', 'N/A'),
                            "asignBdgtAmt": raw_data.get('asignBdgtAmt', 0),
                            "bidNtceDate": raw_data.get('bidNtceDate', ''),
                            "bidClseDate": raw_data.get('bidClseDate', ''),
                            "raw": raw_data,
                            "source": "Vector",
                            "relevance_score": item["similarity"] * 10,
                            "similarity": item["similarity"]
                        }

        combined_results = list(combined_dict.values())
        combined_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        state["combined_results"] = combined_results[:20]

        # ê²€ìƒ‰ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        if combined_results:
            avg_score = np.mean([r["relevance_score"] for r in combined_results[:10]])
            state["quality_score"] = float(avg_score)
        else:
            state["quality_score"] = 0.0

        state["status_messages"] = [f"âœ… {len(state['combined_results'])}ê°œ ê³µê³ ë¡œ í†µí•© ì™„ë£Œ"]

    except Exception as e:
        state["error"] = f"ê²°ê³¼ í†µí•© ì˜¤ë¥˜: {str(e)}"
        state["combined_results"] = []
        state["quality_score"] = 0.0

    return state

def enrich_with_summaries_node(state: BidSearchState) -> BidSearchState:
    """GPT ìš”ì•½ ì¶”ê°€ ë…¸ë“œ"""
    try:
        bid_manager = BidDataManager()
        
        # ê° ê³µê³ ì— ëŒ€í•´ ìš”ì•½ ì •ë³´ ì¶”ê°€
        for result in state["combined_results"]:
            bid_no = result.get("bidNtceNo")
            if bid_no:
                summary, created_at, summary_type = bid_manager.get_bid_summary(bid_no)
                result["summary"] = summary
                result["summary_type"] = summary_type
                result["summary_created_at"] = created_at
        
        state["status_messages"] = ["âœ… GPT ìš”ì•½ ì •ë³´ ì¶”ê°€ ì™„ë£Œ"]
        
    except Exception as e:
        state["error"] = f"ìš”ì•½ ì •ë³´ ì¶”ê°€ ì˜¤ë¥˜: {str(e)}"
        
    return state

def generate_answer_node(state: BidSearchState) -> BidSearchState:
    """AI ë‹µë³€ ìƒì„± ë…¸ë“œ"""
    try:
        if not state["combined_results"]:
            state["final_answer"] = f"'{state['query']}'ì— ëŒ€í•œ ì…ì°° ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return state
        
        # LangChain LLM ì´ˆê¸°í™”
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            api_key=OPENAI_API_KEY
        )
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        contexts = []
        for i, result in enumerate(state["combined_results"][:5]):
            bid_no = result.get("bidNtceNo", "N/A")
            title = result.get("bidNtceNm", "ì œëª© ì—†ìŒ")
            org = result.get("ntceInsttNm", "ê¸°ê´€ëª… ì—†ìŒ")
            summary = result.get("summary", "ìš”ì•½ ì—†ìŒ")

            context_item = f"""
ê³µê³  {i+1}:
- ê³µê³ ë²ˆí˜¸: {bid_no}
- ê³µê³ ëª…: {title}
- ê¸°ê´€: {org}
- ìš”ì•½: {summary}
"""
            contexts.append(context_item)

        context_text = "\n".join(contexts)

        # LangChain ì²´ì¸ ì‹¤í–‰
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ë‹¹ì‹ ì€ ì…ì°° ê³µê³  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            ("human", f"ë‹¤ìŒì€ ì…ì°° ê³µê³  ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤:\n{context_text}\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”: {state['query']}")
        ])

        chain = prompt | llm | StrOutputParser()
        answer = chain.invoke({
            "query": state["query"],
            "context": context_text
        })

        state["final_answer"] = answer
        state["status_messages"] = ["âœ… AI ë¶„ì„ ë‹µë³€ ìƒì„± ì™„ë£Œ!"]

    except Exception as e:
        state["error"] = f"AI ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}"
        state["final_answer"] = "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    
    return state

def check_error(state: BidSearchState) -> str:
    """ì—ëŸ¬ ì²´í¬ ë…¸ë“œ"""
    if state.get("error"):
        return "error"
    return "continue"

# ========== í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë…¸ë“œë“¤ ==========
def search_supabase_hybrid_node(state: HybridSearchState) -> HybridSearchState:
    """Supabase í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë…¸ë“œ"""
    query = state["query"]
    
    state["messages"].append(
        HumanMessage(content=f"Supabase ê²€ìƒ‰ ì‹œì‘: {query}")
    )
    
    try:
        bid_manager = BidDataManager()
        
        # ìµœê·¼ 30ì¼ ë°ì´í„° ê²€ìƒ‰
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=30)).strftime('%Y%m%d')
        
        results = bid_manager.search_bids_by_keyword(
            query, None, start_date, end_date
        )
        
        state["supabase_results"] = results
        state["search_method"].append("Supabase")
        state["messages"].append(
            AIMessage(content=f"Supabaseì—ì„œ {len(results)}ê±´ ê²€ìƒ‰ë¨")
        )
        
    except Exception as e:
        state["messages"].append(
            AIMessage(content=f"Supabase ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        )
        state["supabase_results"] = []
    
    return state

def search_vector_hybrid_node(state: HybridSearchState) -> HybridSearchState:
    """ë²¡í„° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë…¸ë“œ"""
    query = state["query"]
    
    state["messages"].append(
        HumanMessage(content=f"VectorDB ì‹œë§¨í‹± ê²€ìƒ‰ ì‹œì‘")
    )
    
    try:
        semantic_engine = SemanticSearchEngine(OPENAI_API_KEY)
        results = semantic_engine.search(query, num_results=30, similarity_threshold=0.03)
        
        vector_results = []
        for metadata, similarity in results:
            if similarity >= 0.03:
                vector_results.append({
                    "metadata": metadata,
                    "similarity": similarity,
                    "bidNtceNo": metadata.get('ê³µê³ ë²ˆí˜¸', 'N/A'),
                    "bidNtceNm": metadata.get('ê³µê³ ëª…', 'N/A'),
                    "ntceInsttNm": metadata.get('ê¸°ê´€ëª…', 'N/A')
                })
        
        state["vector_results"] = vector_results
        state["search_method"].append("VectorDB")
        state["messages"].append(
            AIMessage(content=f"VectorDBì—ì„œ {len(vector_results)}ê±´ ê²€ìƒ‰ë¨")
        )
        
    except Exception as e:
        state["messages"].append(
            AIMessage(content=f"VectorDB ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        )
        state["vector_results"] = []
    
    return state

def check_need_api_hybrid_node(state: HybridSearchState) -> HybridSearchState:
    """API ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ í™•ì¸"""
    supabase_count = len(state["supabase_results"])
    vector_count = len(state["vector_results"])
    total_count = supabase_count + vector_count
    
    state["messages"].append(
        HumanMessage(content=f"ê²€ìƒ‰ ê²°ê³¼ í™•ì¸: Supabase {supabase_count}ê±´, VectorDB {vector_count}ê±´")
    )
    
    if total_count < 10:
        state["need_api_search"] = True
        state["messages"].append(
            AIMessage(content=f"ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡± ({total_count}ê±´), API ê²€ìƒ‰ í•„ìš”")
        )
    else:
        state["need_api_search"] = False
        state["messages"].append(
            AIMessage(content=f"ì¶©ë¶„í•œ ê²€ìƒ‰ ê²°ê³¼ ({total_count}ê±´), API ê²€ìƒ‰ ë¶ˆí•„ìš”")
        )
    
    return state

def fetch_naratang_api_hybrid_node(state: HybridSearchState) -> HybridSearchState:
    """ë‚˜ë¼ì¥í„° API í˜¸ì¶œ"""
    if not state["need_api_search"]:
        state["api_results"] = {}
        return state
    
    query = state["query"]
    
    state["messages"].append(
        HumanMessage(content=f"ë‚˜ë¼ì¥í„° API ì‹¤ì‹œê°„ ê²€ìƒ‰ ì‹œì‘")
    )
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=30)
    
    all_results = {}
    api_total = 0
    
    for category, endpoint in API_ENDPOINTS.items():
        try:
            state["messages"].append(
                AIMessage(content=f"{category} ì¹´í…Œê³ ë¦¬ API í˜¸ì¶œ ì¤‘...")
            )
            
            params = {
                'serviceKey': API_KEY,
                'pageNo': 1,
                'numOfRows': 20,
                'inqryDiv': 1,
                'type': 'json',
                'inqryBgnDt': start_date.strftime('%Y%m%d') + '0000',
                'inqryEndDt': end_date.strftime('%Y%m%d') + '2359',
                'bidNtceNm': query
            }
            
            query_string = urlencode(params, quote_via=quote_plus)
            url = f"{endpoint}?{query_string}"
            
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                items = data.get('response', {}).get('body', {}).get('items', [])
                
                if items:
                    all_results[category] = items
                    api_total += len(items)
                    state["messages"].append(
                        AIMessage(content=f"{category}: {len(items)}ê±´ ê²€ìƒ‰ ì™„ë£Œ")
                    )
                
        except Exception as e:
            state["messages"].append(
                AIMessage(content=f"{category} API ì˜¤ë¥˜: {str(e)}")
            )
    
    state["api_results"] = all_results
    state["search_method"].append("API")
    state["messages"].append(
        AIMessage(content=f"API ê²€ìƒ‰ ì™„ë£Œ: ì´ {api_total}ê±´")
    )
    
    return state

def combine_hybrid_results_node(state: HybridSearchState) -> HybridSearchState:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ í†µí•©"""
    state["messages"].append(
        HumanMessage(content="ê²€ìƒ‰ ê²°ê³¼ í†µí•© ì¤‘...")
    )
    
    combined_dict = {}
    
    # Supabase ê²°ê³¼ ì¶”ê°€
    for item in state["supabase_results"]:
        bid_no = item.get("bidNtceNo")
        if bid_no:
            combined_dict[bid_no] = {
                **item,
                "source": "Supabase",
                "relevance_score": 5
            }
    
    # VectorDB ê²°ê³¼ ì¶”ê°€/ë³‘í•©
    for item in state["vector_results"]:
        bid_no = item.get("bidNtceNo")
        if bid_no:
            if bid_no in combined_dict:
                combined_dict[bid_no]["relevance_score"] += item["similarity"] * 10
                combined_dict[bid_no]["source"] += ", VectorDB"
            else:
                # Supabaseì—ì„œ ì¶”ê°€ ì •ë³´ ì¡°íšŒ
                bid_manager = BidDataManager()
                bid_detail = bid_manager.get_bid_by_number(bid_no)
                
                if bid_detail:
                    raw_data = bid_detail.get('raw', {})
                    combined_dict[bid_no] = {
                        "bidNtceNo": bid_no,
                        "bidNtceNm": raw_data.get('bidNtceNm', 'N/A'),
                        "ntceInsttNm": raw_data.get('ntceInsttNm', 'N/A'),
                        "source": "VectorDB",
                        "relevance_score": item["similarity"] * 10,
                        "raw": raw_data
                    }
    
    # API ê²°ê³¼ ì¶”ê°€
    for category, items in state["api_results"].items():
        for item in items:
            bid_no = item.get("bidNtceNo")
            if bid_no:
                if bid_no in combined_dict:
                    combined_dict[bid_no]["relevance_score"] += 3
                    combined_dict[bid_no]["source"] += ", API"
                else:
                    combined_dict[bid_no] = {
                        **item,
                        "source": f"API({category})",
                        "relevance_score": 3,
                        "category": category
                    }
    
    combined_results = list(combined_dict.values())
    combined_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
    
    state["combined_results"] = combined_results
    state["total_count"] = len(combined_results)
    
    state["messages"].append(
        AIMessage(content=f"í†µí•© ì™„ë£Œ: ì´ {len(combined_results)}ê°œ")
    )
    
    return state

def generate_hybrid_summary_node(state: HybridSearchState) -> HybridSearchState:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìš”ì•½ ìƒì„±"""
    if not state["combined_results"]:
        state["summary"] = "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        state["final_results"] = []
        return state
    
    state["messages"].append(
        HumanMessage(content="AI ìš”ì•½ ìƒì„± ì¤‘...")
    )
    
    state["final_results"] = state["combined_results"][:20]
    
    # ì†ŒìŠ¤ë³„ í†µê³„
    source_stats = {}
    for item in state["final_results"]:
        sources = item.get("source", "").split(", ")
        for source in sources:
            source_stats[source] = source_stats.get(source, 0) + 1
    
    # ìš”ì•½ ìƒì„±
    summary_parts = [
        f"ğŸ” '{state['query']}' ê²€ìƒ‰ ê²°ê³¼: ì´ {state['total_count']}ê±´",
        f"\nğŸ“Š ê²€ìƒ‰ ì†ŒìŠ¤:"
    ]
    
    for source, count in source_stats.items():
        summary_parts.append(f"  â€¢ {source}: {count}ê±´")
    
    # ìƒìœ„ 3ê°œ ê³µê³  í•˜ì´ë¼ì´íŠ¸
    summary_parts.append(f"\nğŸ† ìƒìœ„ ê³µê³ :")
    for i, item in enumerate(state["final_results"][:3], 1):
        summary_parts.append(
            f"{i}. {item.get('bidNtceNm', 'ì œëª© ì—†ìŒ')[:40]}..."
            f" ({item.get('ntceInsttNm', 'N/A')}, {convert_to_won_format(item.get('asignBdgtAmt', 0))})"
        )
    
    state["summary"] = "\n".join(summary_parts)
    
    state["messages"].append(
        AIMessage(content="ìš”ì•½ ìƒì„± ì™„ë£Œ")
    )
    
    return state

def should_search_api_hybrid(state: HybridSearchState) -> str:
    """API ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
    if state["need_api_search"]:
        return "search_api"
    else:
        return "combine"

# ========== LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ==========
def create_bid_search_workflow():
    """ì…ì°° ê³µê³  ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    workflow = StateGraph(BidSearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("preprocess_query", preprocess_query_node)
    workflow.add_node("search_supabase", search_supabase_node)
    workflow.add_node("search_vector_db", search_vector_db_node)
    workflow.add_node("combine_results", combine_results_node)
    workflow.add_node("enrich_summaries", enrich_with_summaries_node)
    workflow.add_node("generate_answer", generate_answer_node)
    
    # ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("preprocess_query")
    
    # ì¡°ê±´ë¶€ ì—£ì§€
    workflow.add_conditional_edges(
        "preprocess_query",
        check_error,
        {
            "continue": "search_supabase",
            "error": END
        }
    )
    
    workflow.add_edge("search_supabase", "search_vector_db")
    workflow.add_edge("search_vector_db", "combine_results")
    workflow.add_edge("combine_results", "enrich_summaries")
    workflow.add_edge("enrich_summaries", "generate_answer")
    workflow.add_edge("generate_answer", END)
    
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    
    return app

def create_hybrid_search_workflow():
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    workflow = StateGraph(HybridSearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("search_supabase", search_supabase_hybrid_node)
    workflow.add_node("search_vector", search_vector_hybrid_node)
    workflow.add_node("check_need_api", check_need_api_hybrid_node)
    workflow.add_node("search_api", fetch_naratang_api_hybrid_node)
    workflow.add_node("combine", combine_hybrid_results_node)
    workflow.add_node("generate_summary", generate_hybrid_summary_node)
    
    # ì—£ì§€ ì„¤ì •
    workflow.set_entry_point("search_supabase")
    workflow.add_edge("search_supabase", "search_vector")
    workflow.add_edge("search_vector", "check_need_api")
    
    # ì¡°ê±´ë¶€ ì—£ì§€
    workflow.add_conditional_edges(
        "check_need_api",
        should_search_api_hybrid,
        {
            "search_api": "search_api",
            "combine": "combine"
        }
    )
    
    workflow.add_edge("search_api", "combine")
    workflow.add_edge("combine", "generate_summary")
    workflow.add_edge("generate_summary", END)
    
    return workflow.compile()
